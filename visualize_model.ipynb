{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.helper_fcts import display_state\n",
    "from IPython.display import clear_output\n",
    "import os \n",
    "import time\n",
    "from agent_policy.policy_base_class import Policy, DQN_Agent\n",
    "from agent_policy.policy_sac import SAC_Agent\n",
    "from utils.helper_fcts import display_state, class_to_dict, parse_arguments\n",
    "import random, torch, os, time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from envs.environment_t import Environment \n",
    "from agent_policy.policy_base_class import Policy, DQN_Agent \n",
    "from agent_policy.train_policy_cfg import DQNCfg\n",
    "from utils.helper_fcts import class_to_dict, parse_arguments\n",
    "from agent_policy.train import *\n",
    "import sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the name of the model you want here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Ben_6chan_conv_05-06-2023-14-38-01.pt\"\n",
    "# Specify the locaiton of the safe states\n",
    "current_folder = globals()['_dh'][0]\n",
    "project_folder = os.path.dirname(current_folder)\n",
    "model_dir = os.path.join(project_folder,\"model\", filename)\n",
    "### models for variant 0\n",
    "variant = 1  # problem variant\n",
    "mode = \"testing\"\n",
    "\n",
    "\n",
    "# Var 1 \n",
    "# /media/almo/Windows/Users/Thien/Documents/MSNE/SS23_DRLSem/drl_grid_world/model/30-07-2023-18-04-57_var1/BIG_SAC_Conv1_1_4duel_var_1_30-07-2023-18-04-57_actorbest_.pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/almo/Windows/Users/Thien/Documents/MSNE/SS23_DRLSem/drl_grid_world/model/30-07-2023-18-04-57_var1/BIG_SAC_Conv1_1_4duel_var_1_30-07-2023-18-04-57_actorbest_.pt\n"
     ]
    }
   ],
   "source": [
    "model_dir = input()\n",
    "print(model_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your agent there. Sadly, you would need to recreate the agent so that you the saved model will have a shell to paste into. The easiest way i found is finding the hyperparams on wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<agent_policy.policy_sac.SAC_Agent at 0x7f0421533f10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "policy_name = 'sac'\n",
    "neural_net_type = 'duel' # choose between 'conv' and 'fc' and 'duel'\n",
    "device=\"cuda\"\n",
    "# pre_trained_model = None # leave to None if we're not doing transfert learning\n",
    "### problem variant\n",
    "\n",
    "## END OF PARAMETERS ##\n",
    "\n",
    "# other parameters \n",
    "data_dir = './data'\n",
    "policy_dict = {\"dqn\": 'DQN_Agent', \"sac\": 'SAC_Agent'} \n",
    "variable_lr = False # schedule for the learning rate\n",
    "\n",
    "### seeding ###\n",
    "seed = 1  # set seed to allow for reproducibility of results\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed=seed)\n",
    "\n",
    "args = parse_arguments([])\n",
    "obs_model = args.obs_model\n",
    "\n",
    "if neural_net_type == 'conv' or neural_net_type == 'duel':\n",
    "    layers_dims = None\n",
    "    conv_architecture = args.conv_arch\n",
    "    n_obs = (1, conv_architecture['num_channel'], 5, 5)\n",
    "    env = Environment(variant, \n",
    "                      data_dir, \n",
    "                      neural_net_type=neural_net_type, \n",
    "                    #   obs_model=obs_model\n",
    "                      )\n",
    "    state = env.reset(mode)\n",
    "    \n",
    "policy_class = eval(policy_dict[policy_name])\n",
    "agent: Policy = policy_class(args, \n",
    "                            #   num_inputs=n_obs, \n",
    "                            #   num_outputs=args.n_actions,\n",
    "                            #   device=device,\n",
    "                            #   layer_dims=layers_dims,\n",
    "                            #   activation='relu',  \n",
    "                            #   weight_init=True,\n",
    "                            # #  pre_trained_model=pre_trained_model,\n",
    "                            #   conv_architecture=conv_architecture,\n",
    "                            #   variable_lr=False,\n",
    "                            #   dropout_rate=args.dropout_rate,\n",
    "                            #   net_type=neural_net_type,\n",
    "                              )\n",
    "\n",
    "\n",
    "saved_model = torch.load(model_dir)\n",
    "agent.actor.load_state_dict(saved_model)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorNetwork(\n",
       "  (activation): ReLU()\n",
       "  (feature_ex_block): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 32, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (linear_block): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=5, bias=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.actor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the newer code visualize the model with the new way of saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new episode #  99  with reward: 354.5 \n",
      "312.61\n"
     ]
    }
   ],
   "source": [
    "model = agent\n",
    "add_viz = False\n",
    "test_rew = 0. \n",
    "eps_rew = []\n",
    "env.test_episode_counter = 0\n",
    "\n",
    "for idx in range(100):\n",
    "    obs = env.reset(mode)\n",
    "    state = obs.to(device)\n",
    "    test_rew = 0\n",
    "    for j in range(201):  # loop over 200 steps per episode\n",
    "        model.actor.eval()\n",
    "        act = model.select_action(state, greedy=False)  # TODO: get action for the obs from your trained policy\n",
    "        rew, next_obs, done = env.step(act)  # take one step in the environment\n",
    "\n",
    "        test_rew += rew\n",
    "        state = next_obs.to(device)\n",
    "        # visualization\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if add_viz:\n",
    "            display_state(state=env.get_state(), episode=mode, time =env.step_count,episode_nr=idx, total_rew=test_rew)\n",
    "            time.sleep(0.0001)  \n",
    "    eps_rew.append(test_rew)\n",
    "    if not add_viz:\n",
    "        print(\"new episode # \", idx, f' with reward: {eps_rew[idx]} ',  )\n",
    "\n",
    "avg_test_rew = np.sum(eps_rew) / 100  # compute the average reward per episode\n",
    "\n",
    "print(avg_test_rew) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195.095"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_test_rew\n",
    "## Var0: 182\n",
    "## Var1: 316 \n",
    "## Var2: 195\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
