{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'agent_policy.policy_cfg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39magent_policy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpolicy_base_class\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39magent_policy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpolicy_cfg\u001b[39;00m \u001b[39mimport\u001b[39;00m DQNCfg\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataloader\u001b[39;00m \u001b[39mimport\u001b[39;00m GridDataLoader\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'agent_policy.policy_cfg'"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "import random\n",
    "random.seed(seed)\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "import torch\n",
    "torch.manual_seed(seed)\n",
    "from envs.environment_t import Environment \n",
    "from utils.helper_fcts import display_state, class_to_dict\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torch import Tensor, nn\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "import random\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from itertools import compress, chain\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import time \n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from agent_policy.policy_base_class import *\n",
    "from agent_policy.policy_cfg import DQNCfg\n",
    "from utils.dataloader import GridDataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING CONFIG ###\n",
    "wandb_run_name = 'thien_dev'\n",
    "cfg=DQNCfg\n",
    "cfg_dict=class_to_dict(cfg)\n",
    "# cfg_dict['training']['BATCH_SIZE']\n",
    "\n",
    "BATCH_SIZE = cfg_dict['training']['BATCH_SIZE']\n",
    "GAMMA = cfg_dict['training']['GAMMA']\n",
    "EPS_START = cfg_dict['training']['EPS_START']\n",
    "EPS_END = cfg_dict['training']['EPS_END']\n",
    "EPS_DECAY = cfg_dict['training']['EPS_DECAY']\n",
    "TAU = cfg_dict['training']['TAU']\n",
    "LR = cfg_dict['training']['LR']\n",
    "num_episodes = cfg_dict['training']['NUM_EPISODES']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "### Init environment & agent ####\n",
    "mode = 'training'\n",
    "data_dir = os.path.join(os.path.abspath(\"\"), \"data\")\n",
    "variant = 0  \n",
    "env = Environment(variant, data_dir)\n",
    "dl = GridDataLoader(variant, mode, 20000)\n",
    "# Get number of actions from gym action space\n",
    "n_actions = 5\n",
    "# Get the number of state observations\n",
    "state = env.reset(mode)\n",
    "n_obs = len(state)\n",
    "print(n_obs)\n",
    "num_layers = 10\n",
    "layers_dims = [128 for i in range(num_layers)]\n",
    "\n",
    "dqn_agent = DQN_Agent(num_inputs=n_obs, num_outputs=n_actions, layer_dims=layers_dims, activation='lrelu', device=device)\n",
    "\n",
    "# optimizer = Adam(dqn_agent.policy.parameters(), lr=LR, amsgrad=True)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight and biases setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbenoitauclair30\u001b[0m (\u001b[33mitsonlygrid\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/benoit/Documents/00_git/drl_grid_world/wandb/run-20230515_115738-ipbgbw3v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itsonlygrid/Warehouse%20Gridworld/runs/ipbgbw3v' target=\"_blank\">thien_dev</a></strong> to <a href='https://wandb.ai/itsonlygrid/Warehouse%20Gridworld' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itsonlygrid/Warehouse%20Gridworld' target=\"_blank\">https://wandb.ai/itsonlygrid/Warehouse%20Gridworld</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itsonlygrid/Warehouse%20Gridworld/runs/ipbgbw3v' target=\"_blank\">https://wandb.ai/itsonlygrid/Warehouse%20Gridworld/runs/ipbgbw3v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/itsonlygrid/Warehouse%20Gridworld/runs/ipbgbw3v?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0be6fbd8d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Warehouse Gridworld\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config=cfg_dict['training'],\n",
    "    name=wandb_run_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning loop   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition = namedtuple('Transition',('state','action','next_state','reward'))\n",
    "\n",
    "# def optimize_model(): \n",
    "#     if len(dl.memory) < BATCH_SIZE: \n",
    "#         return \n",
    "    \n",
    "#     # turn the transition to batch \n",
    "#     transitions = dl.sample(BATCH_SIZE)\n",
    "#     # print(f\" Sample Transition {transitions[15]}\")\n",
    "#     batch = Transition(*zip(*transitions))\n",
    "#     # print(f\" Batch for training {batch.action}\")\n",
    "#     non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "#                                           batch.next_state)), device=device, dtype=torch.bool)\n",
    "#     non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "#                                                 if s is not None])\n",
    "#     state_batch = torch.cat(batch.state)\n",
    "#     action_batch = torch.cat(batch.action)\n",
    "#     reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "#     # Compute Q(s_t, a)\n",
    "#     state_action_values = dqn_agent.policy(state_batch).gather(1, action_batch)\n",
    "#     # --> get the output of the net using the state, where an action was taken\n",
    "#     # \n",
    "    \n",
    "#     # Comput Vs for all nextstate \n",
    "#     next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "#     with torch.no_grad(): \n",
    "#         next_state_values[non_final_mask] = dqn_agent.target(non_final_next_states).max(1)[0]\n",
    "        \n",
    "#     expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "#     # Compute Huber Loss \n",
    "#     criterion = nn.MSELoss()\n",
    "#     loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "#     # Optimize the model \n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "    \n",
    "#     # Inplace gradient clipping \n",
    "#     # torch.nn.utils.clip_grad_value_(dqn_agent.policy.parameters(), 100)\n",
    "#     optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start, running on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                 | 0/10000 [00:36<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m dqn_agent\u001b[38;5;241m.\u001b[39msave_experience(state, action, next_state, reward)\n\u001b[1;32m     45\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m---> 47\u001b[0m training_loss \u001b[38;5;241m=\u001b[39m dqn_agent\u001b[38;5;241m.\u001b[39moptimize()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Upate target network \u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_done \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Soft update of the target network \u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/00_git/drl_grid_world/agent_policy/policy_base_class.py:154\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m # Sample from the experience replace \n\u001b[1;32m    152\u001b[0m state_batch, action_batch, reward_batch, non_final_mask, non_final_next_states = self.sample_from_experience(self.BATCH_SIZE)\n\u001b[0;32m--> 154\u001b[0m # Compute Q(s_t, a)\n\u001b[1;32m    155\u001b[0m state_action_values = self.policy(state_batch).gather(1, action_batch)\n\u001b[1;32m    157\u001b[0m # Compute V for all next states\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "cummulative_reward = []\n",
    "\n",
    "print(f'Training Start, running on {device}')\n",
    "\n",
    "pbar = tqdm(total=num_episodes)\n",
    "\n",
    "step_done = 0 \n",
    "for i_expisode in range(num_episodes):\n",
    "    \n",
    "    ## Run Validation Reward\n",
    "    if i_expisode % 100 == 0 : \n",
    "        step_done += 0 \n",
    "        val_rew = 0 \n",
    "        state = env.reset(\"validation\")\n",
    "        state = torch.tensor(state, dtype=torch.float32, device = device).unsqueeze(0)\n",
    "        for j in range(200):  # loop over 200 steps per episode\n",
    "            act = dqn_agent.select_action(state, greedy=True)  # TODO: get action for the obs from your trained policy\n",
    "            rew, next_obs, _ = env.step(act)  # take one step in the environment\n",
    "            val_rew += rew  # track rewards\n",
    "            obs = next_obs  # continue from the new obs\n",
    "        wandb.log({\n",
    "            \"val_reward\": val_rew\n",
    "        })    \n",
    "        # pbar.write(f'Episode {i_expisode} / {num_episodes}, validation reward: {val_rew}')\n",
    "\n",
    "    # Training LOOP\n",
    "    state = env.reset(mode)\n",
    "    state = torch.tensor(state, dtype=torch.float32, device = device).unsqueeze(0)\n",
    "    \n",
    "    training_reward  = 0.0\n",
    "    for t in count():\n",
    "        action = dqn_agent.select_action(state)\n",
    "        reward, observation, done = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        training_reward += reward\n",
    "        if done: \n",
    "            next_state = None\n",
    "        else:             \n",
    "            next_state = torch.tensor(observation, dtype=torch.float32,device=device).unsqueeze(0)\n",
    "    \n",
    "        # store in memory \n",
    "        # dl.push(state, action, next_state, reward)\n",
    "        dqn_agent.save_experience(state, action, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "        training_loss = dqn_agent.optimize()\n",
    "        \n",
    "        # Upate target network \n",
    "        if step_done % 400 == 0:\n",
    "            # Soft update of the target network \n",
    "            # θ′ ← τ θ + (1 −τ )θ′\n",
    "            target_net_state_dict = dqn_agent.target.state_dict()\n",
    "            policy_net_state_dict = dqn_agent.policy.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*TAU \\\n",
    "                    + target_net_state_dict[key]*(1-TAU)\n",
    "            dqn_agent.target.load_state_dict(target_net_state_dict)\n",
    "            \n",
    "        \n",
    "        if done: \n",
    "            wandb.log({\n",
    "                \"training_reward\" : training_reward,\n",
    "                \"training_loss\": training_loss\n",
    "            })\n",
    "            break\n",
    "    pbar.update(1) \n",
    "#     scheduler.step(val_rew)\n",
    "pbar.close()\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar.close()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game with the final state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset(\"validation\")\n",
    "state = torch.tensor(state, dtype=torch.float32, device = device).unsqueeze(0)\n",
    "\n",
    "for j in range(200):  # loop over 200 steps per episode\n",
    "    \n",
    "    act = dqn_agent.select_action(state)  # TODO: get action for the obs from your trained policy\n",
    "    rew, next_obs, _ = env.step(act)  # take one step in the environment\n",
    "    # visualization\n",
    "    clear_output(wait=True)\n",
    "    print(act)\n",
    "    display_state(state=env.get_state(), episode=\"Validation \")\n",
    "    time.sleep(0.2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
